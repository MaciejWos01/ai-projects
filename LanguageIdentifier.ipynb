{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MVH1RUT5eNRW"
      },
      "outputs": [],
      "source": [
        "from typing import Iterable\n",
        "import requests\n",
        "import tarfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from typing import Any\n",
        "import unittest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UCeeQJBceys2"
      },
      "outputs": [],
      "source": [
        "def filter_language(members: tarfile.TarFile,\n",
        "                    language: str) -> Iterable[tarfile.TarInfo]:\n",
        "    for tarinfo in members:\n",
        "        if os.path.splitext(tarinfo.name)[1] == \".\"+language:\n",
        "            yield tarinfo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "505rm8Qiifx8"
      },
      "source": [
        "Creating a dictionary of all considered languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dARFag4Ieypr"
      },
      "outputs": [],
      "source": [
        "languages = ['bg', 'cs', 'da', 'de', 'el', 'en', 'es', 'et', 'fi', 'fr', 'hu',\n",
        "             'it', 'lt', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']\n",
        "language_names = {\n",
        "    'bg': 'Bulgarian',\n",
        "    'cs': 'Czech',\n",
        "    'da': 'Danish',\n",
        "    'de': 'German',\n",
        "    'el': 'Greek',\n",
        "    'en': 'English',\n",
        "    'es': 'Spanish',\n",
        "    'et': 'Estonian',\n",
        "    'fi': 'Finnish',\n",
        "    'fr': 'French',\n",
        "    'hu': 'Hungarian',\n",
        "    'it': 'Italian',\n",
        "    'lt': 'Lithuanian',\n",
        "    'lv': 'Latvian',\n",
        "    'nl': 'Dutch',\n",
        "    'pl': 'Polish',\n",
        "    'pt': 'Portuguese',\n",
        "    'ro': 'Romanian',\n",
        "    'sk': 'Slovak',\n",
        "    'sl': 'Slovenian',\n",
        "    'sv': 'Swedish'\n",
        "}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TxhGe5VIjDpk"
      },
      "source": [
        "Downloading and unpacking documents for all considered languages from EU\n",
        "website which are transleted versions of the set of the same *documents*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vD_gfHwXeynt"
      },
      "outputs": [],
      "source": [
        "for language in languages:\n",
        "    if language != 'en':\n",
        "        url = \"https://www.statmt.org/europarl/v7/{}-en.tgz\".format(language)\n",
        "        response = requests.get(url, stream=True)\n",
        "        tar = tarfile.open(fileobj=response.raw, mode=\"r|gz\")\n",
        "        tar.extractall(members=filter_language(tar, language))\n",
        "        tar.close()\n",
        "    else:\n",
        "        url = \"https://www.statmt.org/europarl/v7/bg-en.tgz\"\n",
        "        response = requests.get(url, stream=True)\n",
        "        tar = tarfile.open(fileobj=response.raw, mode=\"r|gz\")\n",
        "        tar.extractall(members=filter_language(tar, language))\n",
        "        tar.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XuqLCag4kQQP"
      },
      "source": [
        "Connecting every document with its language and merging them into one dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHWMdo0oeykh"
      },
      "outputs": [],
      "source": [
        "huge_df = pd.DataFrame([])\n",
        "for language, language_name in language_names.items():\n",
        "    if language == 'en':\n",
        "        huge_df[language_name] = pd.read_csv(\n",
        "            \"europarl-v7.bg-en.en\", \"utf-8\", header=None,\n",
        "            names=[language_name], engine='python')\n",
        "    else:\n",
        "        huge_df[language_name] = pd.read_csv(\n",
        "            \"europarl-v7.{}-en.{}\".format(language, language),\n",
        "            \"utf-8\", header=None, names=[language_name], engine='python')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_HxnRI6ClBeK"
      },
      "source": [
        "Function which given a line of text converts it to lowercase and getting rid of digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Re37f8Jkeyie"
      },
      "outputs": [],
      "source": [
        "def uniform(line: Any) -> Any:\n",
        "    if line is not None and len(line) != 0:\n",
        "        line = line.lower()\n",
        "        line = re.sub(r\"\\d+\", \"\", line)\n",
        "        return line\n",
        "    else:\n",
        "        None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OgOxSmHeyfM"
      },
      "outputs": [],
      "source": [
        "huge_uniform_df = huge_df.replace(r'[^\\w\\s]', '', regex=True).astype(str)\n",
        "huge_uniform_df = huge_uniform_df.applymap(uniform)\n",
        "huge_uniform_df_melted = pd.melt(huge_uniform_df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pR8kDWUMloSd"
      },
      "source": [
        "Extracting the independent and dependent variable out of\n",
        "dataframe and reducing number of variables due to limited RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbbGZjiVeydO"
      },
      "outputs": [],
      "source": [
        "X = huge_uniform_df_melted[\"value\"]\n",
        "Y = huge_uniform_df_melted[\"variable\"]\n",
        "X = X[::700]\n",
        "Y = Y[::700]\n",
        "le = LabelEncoder()\n",
        "Y = le.fit_transform(Y)\n",
        "X = np.array(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWjS5CWreyaB"
      },
      "outputs": [],
      "source": [
        "data_list = []\n",
        "for i in range(len(X)):\n",
        "    text = str(X[i]).lower()\n",
        "    data_list.append(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9x1SbjieyYO"
      },
      "outputs": [],
      "source": [
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(data_list).toarray()\n",
        "\n",
        "dataframe_vocabulary = pd.DataFrame(X.toarray(), columns=cv.vocabulary_)\n",
        "dataframe_vocabulary[:5]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GxeWYJclnxlK"
      },
      "source": [
        "Train - test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fu1SC_F-fG1m"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.20)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBkjh-OnoWOn"
      },
      "source": [
        "Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBlO_IPmfGyB"
      },
      "outputs": [],
      "source": [
        "model = MultinomialNB()\n",
        "model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt7vBm-5fGwa"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(x_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F8jeLbxqoiEr"
      },
      "source": [
        "Training metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYF_i3BJfGtR"
      },
      "outputs": [],
      "source": [
        "ac = accuracy_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB94Ta7afGrX"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy is :\", ac)\n",
        "cm_df = pd.DataFrame(cm, index=languages, columns=languages)\n",
        "plt.figure(figsize=(20, 15))\n",
        "sns.heatmap(cm_df, annot=True, cmap=sns.cubehelix_palette(as_cmap=True))\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPEUqC8UrTpl"
      },
      "source": [
        "Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQAd3PEnrZuw"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sentence: str) -> np.ndarray:\n",
        "    processed_sentence = uniform(sentence)\n",
        "    processed_sentence = cv.transform([processed_sentence]).toarray()\n",
        "    return processed_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ze-5pNL5rZsA"
      },
      "outputs": [],
      "source": [
        "def predict_language(sentence: str) -> str:\n",
        "    processed_sentence = preprocess_sentence(sentence)\n",
        "    predicted_label = le.inverse_transform(\n",
        "        model.predict(processed_sentence))[0]\n",
        "    return predicted_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLjNJqZwrTMf"
      },
      "outputs": [],
      "source": [
        "input_sentence = \"Exemplary sentence\"\n",
        "predicted_language = predict_language(input_sentence)\n",
        "print(\"Predicted Language:\", predicted_language)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j_TTfdAXvz0k"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiWM0D_0v1MS"
      },
      "outputs": [],
      "source": [
        "class TestNotebook(unittest.TestCase):\n",
        "\n",
        "    def test_add(self):\n",
        "        self.assertEqual(uniform(\"To jest 1. Unit Test.\"), \"to jest unit test\")\n",
        "\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
